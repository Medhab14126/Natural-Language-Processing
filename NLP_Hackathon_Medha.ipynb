{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "NMT_with_Attention.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eqoRnaH7q0ww"
      },
      "source": [
        "Proprietary content. ©Great Learning. All Rights Reserved. Unauthorized use or distribution prohibited"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "31aZw0exxyl5"
      },
      "source": [
        "Source: https://www.tensorflow.org/tutorials/text/nmt_with_attention<br>\n",
        " license: Apache License 2.0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3yfTF1xn9zBQ"
      },
      "source": [
        "### Libraries along with their versions used at the time of making notebook-\n",
        "google\t2.0.3\n",
        "\n",
        "matplotlib\t3.1.3\n",
        "\n",
        "numpy\t1.18.1\n",
        "\n",
        "tensorflow\t2.1.0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qZnoPEDblG2L"
      },
      "source": [
        "# Neural machine translation with attention"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZOk8Eu4_t70R"
      },
      "source": [
        "Firstly, let's select TensorFlow version 2.x in colab"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "H6RZUm0p4wYJ",
        "outputId": "bb58009e-6474-4b97-ca9c-d8c0c84d2257"
      },
      "source": [
        "%tensorflow_version 2.x\n",
        "import tensorflow as tf\n",
        "tf.__version__"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'2.7.0'"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TWi96z-8SyX0"
      },
      "source": [
        "# Initialize the random number generator\n",
        "import random\n",
        "random.seed(0)\n",
        "\n",
        "# Ignore the warnings\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OKVkkLaLdPGU"
      },
      "source": [
        "### Load the dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JK0ZRJoxhrFe"
      },
      "source": [
        "As we are using google colab, we need to mount the google drive to load the data file"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dv6RCAcGLd4w",
        "outputId": "59839cb2-50c7-4039-c3df-8b88caa05f35"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3gqIEcuK-gqQ"
      },
      "source": [
        "Add path to the file"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fzXrTG6fLqw4"
      },
      "source": [
        "path_to_file = '/content/drive/MyDrive/TrainData.txt'"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Otv_nAznld5e"
      },
      "source": [
        "Let's convert unicode file to ascii"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zI-7t7PQlZYS"
      },
      "source": [
        "import unicodedata\n",
        "\n",
        "# Converts the unicode file to ascii\n",
        "def unicode_to_ascii(s):\n",
        "    return ''.join(c for c in unicodedata.normalize('NFD', s)\n",
        "        if unicodedata.category(c) != 'Mn')"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HTL5gSMYlqg0"
      },
      "source": [
        "Preprocessing the sentence"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XhX_e_sHxymJ"
      },
      "source": [
        "import re\n",
        "\n",
        "def preprocess_sentence(w):\n",
        "    w = unicode_to_ascii(w.lower().strip())\n",
        "    \n",
        "    # creating a space between a word and the punctuation following it\n",
        "    # eg: \"he is a boy.\" => \"he is a boy .\" \n",
        "    # Reference:- https://stackoverflow.com/questions/3645931/python-padding-punctuation-with-white-spaces-keeping-punctuation\n",
        "    w = re.sub(r\"([?.!,¿])\", r\" \\1 \", w)\n",
        "    w = re.sub(r'[\" \"]+', \" \", w)\n",
        "    \n",
        "    # replacing everything with space except (a-z, A-Z, \".\", \"?\", \"!\", \",\")\n",
        "    #w = re.sub(r\"[^a-zA-Z?.!,¿]+\", \" \", w) # COMMENT THIS LINE FOR NON-LATIN SCRIPTS SUCH AS MARATHI, HINDI ETC.\n",
        "    \n",
        "    w = w.rstrip().strip()\n",
        "    \n",
        "    # adding a start and an end token to the sentence\n",
        "    # so that the model know when to start and stop predicting.\n",
        "    w = '<start> ' + w + ' <end>'\n",
        "    return w"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jDGj9dw5l4xb"
      },
      "source": [
        "Now let's define a function to create the dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_Dt0BSlUxymL"
      },
      "source": [
        "# 1. Remove the accents\n",
        "# 2. Clean the sentences\n",
        "# 3. Return word pairs in the format: [ENGLISH, MARATHI]\n",
        "def create_dataset(path, num_examples):\n",
        "    lines = open(path, encoding='UTF-8').read().strip().split('\\n')\n",
        "    \n",
        "    word_pairs = [[preprocess_sentence(w) for w in l.split('\\t')]  for l in lines[:num_examples]]\n",
        "    \n",
        "    return word_pairs"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dkGkk9l0pgO5"
      },
      "source": [
        "Let's create data for 10 examples to visualize"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H23qgp5ipqQ-",
        "outputId": "1daea8d6-6a26-44fd-b5b7-e1c08d685f36"
      },
      "source": [
        "create_dataset(path_to_file, num_examples=10)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[['<start> answer <end>', '<start> question <end>'],\n",
              " [\"<start> yes . it's absolutely beautiful today . <end>\",\n",
              "  \"<start> the weather is great isn't it ? <end>\"],\n",
              " ['<start> yes , i like that one , too . <end>',\n",
              "  \"<start> that one . the one that's all black . <end>\"],\n",
              " [\"<start> it's really nice . <end>\", \"<start> i got it from macy's . <end>\"],\n",
              " ['<start> at 8:00 p . m . <end>', '<start> when does it start ? <end>'],\n",
              " ['<start> nothing , except my favorite color is blue . <end>',\n",
              "  \"<start> what's the matter with green eyes ? <end>\"],\n",
              " ['<start> i have read just about everything in project gutenberg . <end>',\n",
              "  '<start> have you ever read a book <end>'],\n",
              " ['<start> a fancy name for applied computer science in biology . <end>',\n",
              "  '<start> what is bioinformatics <end>'],\n",
              " ['<start> sure , the changing rooms are over there . <end>',\n",
              "  '<start> can i try it on ? <end>'],\n",
              " ['<start> i want it taken from my checking account . <end>',\n",
              "  '<start> which account are you making this withdrawal from ? <end>']]"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f0R5z1m2mwJE"
      },
      "source": [
        "Define a class to create a word -> index mapping"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OPP74SBMxymN"
      },
      "source": [
        "# This class creates a word -> index mapping (e.g,. \"dad\" -> 5) and vice-versa \n",
        "# (e.g., 5 -> \"dad\") for each language,\n",
        "class LanguageIndex():\n",
        "  def __init__(self, lang):\n",
        "    self.lang = lang\n",
        "    self.word2idx = {}\n",
        "    self.idx2word = {}\n",
        "    self.vocab = set()\n",
        "    \n",
        "    self.create_index()\n",
        "    \n",
        "  def create_index(self):\n",
        "    for phrase in self.lang:\n",
        "      self.vocab.update(phrase.split(' '))\n",
        "    \n",
        "    self.vocab = sorted(self.vocab)\n",
        "    \n",
        "    self.word2idx['<pad>'] = 0\n",
        "    for index, word in enumerate(self.vocab):\n",
        "      self.word2idx[word] = index + 1\n",
        "    \n",
        "    for word, index in self.word2idx.items():\n",
        "      self.idx2word[index] = word"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vt0IMZ_UxymS"
      },
      "source": [
        "def max_length(tensor):\n",
        "    return max(len(t) for t in tensor)\n",
        "\n",
        "\n",
        "def load_dataset(path, num_examples):\n",
        "    # creating cleaned input, output pairs\n",
        "    pairs = create_dataset(path, num_examples)\n",
        "\n",
        "    # index language using the class defined above    \n",
        "    inp_lang = LanguageIndex(mr for en, mr in pairs)\n",
        "    targ_lang = LanguageIndex(en for en, mr in pairs)\n",
        "    \n",
        "    # Vectorize the input and target languages\n",
        "    \n",
        "    # Other language sentences\n",
        "    input_tensor = [[inp_lang.word2idx[s] for s in mr.split(' ')] for en, mr in pairs]\n",
        "    \n",
        "    # English sentences\n",
        "    target_tensor = [[targ_lang.word2idx[s] for s in en.split(' ')] for en, mr in pairs]\n",
        "    \n",
        "    # Calculate max_length of input and output tensor\n",
        "    # Here, we'll set those to the longest sentence in the dataset\n",
        "    max_length_inp, max_length_tar = max_length(input_tensor), max_length(target_tensor)\n",
        "    \n",
        "    # Padding the input and output tensor to the maximum length\n",
        "    input_tensor = tf.keras.preprocessing.sequence.pad_sequences(input_tensor, \n",
        "                                                                 maxlen=max_length_inp,\n",
        "                                                                 padding='post')\n",
        "    \n",
        "    target_tensor = tf.keras.preprocessing.sequence.pad_sequences(target_tensor, \n",
        "                                                                  maxlen=max_length_tar, \n",
        "                                                                  padding='post')\n",
        "    \n",
        "    return input_tensor, target_tensor, inp_lang, targ_lang, max_length_inp, max_length_tar"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fXnIH0SExymU"
      },
      "source": [
        "# Try experimenting with the size of that dataset\n",
        "num_examples = 30000\n",
        "input_tensor, target_tensor, inp_lang, targ_lang, max_length_inp, max_length_targ = load_dataset(path_to_file, num_examples)"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KWihipVfxymZ",
        "outputId": "462881ff-fe54-4916-a431-5e585124bf57"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Creating training and validation sets using an 90-10 split\n",
        "input_tensor_train, input_tensor_val, target_tensor_train, target_tensor_val = train_test_split(input_tensor, target_tensor, test_size=0.1)\n",
        "\n",
        "# Show length\n",
        "len(input_tensor_train), len(target_tensor_train), len(input_tensor_val), len(target_tensor_val)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(4946, 4946, 550, 550)"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MEpK4kKCxymb"
      },
      "source": [
        "BUFFER_SIZE = len(input_tensor_train)\n",
        "BATCH_SIZE = 64\n",
        "N_BATCH = BUFFER_SIZE//BATCH_SIZE\n",
        "embedding_dim = 256\n",
        "units = 1024\n",
        "vocab_inp_size = len(inp_lang.word2idx)\n",
        "vocab_tar_size = len(targ_lang.word2idx)\n",
        "\n",
        "dataset = tf.data.Dataset.from_tensor_slices((input_tensor_train, target_tensor_train)).shuffle(BUFFER_SIZE)\n",
        "dataset = dataset.batch(BATCH_SIZE, drop_remainder=True)"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yfo1hnK1xymi"
      },
      "source": [
        "class Encoder(tf.keras.Model):\n",
        "    def __init__(self, vocab_size, embedding_dim, enc_units, batch_sz):\n",
        "        super(Encoder, self).__init__()\n",
        "        self.batch_sz = batch_sz\n",
        "        self.enc_units = enc_units\n",
        "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
        "        self.gru = tf.keras.layers.GRU(self.enc_units, \n",
        "                                    return_sequences=True, \n",
        "                                    return_state=True, \n",
        "                                    recurrent_initializer='glorot_uniform')\n",
        "        \n",
        "    def call(self, x, hidden):\n",
        "        x = self.embedding(x)\n",
        "        output, state = self.gru(x, initial_state = hidden)        \n",
        "        return output, state\n",
        "    \n",
        "    def initialize_hidden_state(self):\n",
        "        return tf.zeros((self.batch_sz, self.enc_units))"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AlRHWiWoxyml"
      },
      "source": [
        "class Decoder(tf.keras.Model):\n",
        "    def __init__(self, vocab_size, embedding_dim, dec_units, batch_sz):\n",
        "        super(Decoder, self).__init__()\n",
        "        self.batch_sz = batch_sz\n",
        "        self.dec_units = dec_units\n",
        "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
        "        self.gru = tf.keras.layers.GRU(self.dec_units, \n",
        "                                    return_sequences=True, \n",
        "                                    return_state=True, \n",
        "                                    recurrent_initializer='glorot_uniform')\n",
        "        \n",
        "        self.fc = tf.keras.layers.Dense(vocab_size)\n",
        "        \n",
        "        # used for attention\n",
        "        self.W1 = tf.keras.layers.Dense(self.dec_units)\n",
        "        self.W2 = tf.keras.layers.Dense(self.dec_units)\n",
        "        self.V = tf.keras.layers.Dense(1)\n",
        "        \n",
        "    def call(self, x, hidden, enc_output):\n",
        "        # enc_output shape == (batch_size, max_length, hidden_size)\n",
        "        \n",
        "        # hidden shape == (batch_size, hidden size)\n",
        "        # hidden_with_time_axis shape == (batch_size, 1, hidden size)\n",
        "        # we are doing this to perform addition to calculate the score\n",
        "        hidden_with_time_axis = tf.expand_dims(hidden, 1)\n",
        "        \n",
        "        # score shape == (batch_size, max_length, 1)\n",
        "        # we get 1 at the last axis because we are applying tanh(FC(EO) + FC(H)) to self.V\n",
        "        score = self.V(tf.nn.tanh(self.W1(enc_output) + self.W2(hidden_with_time_axis)))\n",
        "        \n",
        "        # attention_weights shape == (batch_size, max_length, 1)\n",
        "        attention_weights = tf.nn.softmax(score, axis=1)\n",
        "        \n",
        "        # context_vector shape after sum == (batch_size, hidden_size)\n",
        "        context_vector = attention_weights * enc_output\n",
        "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
        "        \n",
        "        # x shape after passing through embedding == (batch_size, 1, embedding_dim)\n",
        "        x = self.embedding(x)\n",
        "        \n",
        "        # x shape after concatenation == (batch_size, 1, embedding_dim + hidden_size)\n",
        "        x = tf.concat([tf.expand_dims(context_vector, 1), x], axis=-1)\n",
        "        \n",
        "        # passing the concatenated vector to the GRU\n",
        "        output, state = self.gru(x)\n",
        "        \n",
        "        # output shape == (batch_size * 1, hidden_size)\n",
        "        output = tf.reshape(output, (-1, output.shape[2]))\n",
        "        \n",
        "        # output shape == (batch_size * 1, vocab)\n",
        "        x = self.fc(output)\n",
        "        \n",
        "        return x, state, attention_weights\n",
        "        \n",
        "    def initialize_hidden_state(self):\n",
        "        return tf.zeros((self.batch_sz, self.dec_units))"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "msK13dSFxymq"
      },
      "source": [
        "encoder = Encoder(vocab_inp_size, embedding_dim, units, BATCH_SIZE)\n",
        "decoder = Decoder(vocab_tar_size, embedding_dim, units, BATCH_SIZE)"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dDcm4Mulxyms"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "def loss_function(real, pred):\n",
        "  mask = 1 - np.equal(real, 0)\n",
        "  loss_ = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=real, logits=pred) * mask\n",
        "  return tf.reduce_mean(loss_)"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cn4cwM-wxymu"
      },
      "source": [
        "import os\n",
        "optimizer = tf.optimizers.Adam()\n",
        "\n",
        "checkpoint_dir = './training_checkpoints'\n",
        "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")\n",
        "checkpoint = tf.train.Checkpoint(optimizer=optimizer,\n",
        "                                 encoder=encoder,\n",
        "                                 decoder=decoder)"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sSaBu5vWxymz",
        "outputId": "800cde9b-f362-4329-eeb7-f2ff1809fdda"
      },
      "source": [
        "import time\n",
        "\n",
        "EPOCHS = 70\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "    start = time.time()\n",
        "    \n",
        "    hidden = encoder.initialize_hidden_state()\n",
        "    total_loss = 0\n",
        "    \n",
        "    for (batch, (inp, targ)) in enumerate(dataset):\n",
        "        loss = 0\n",
        "        \n",
        "        with tf.GradientTape() as tape:\n",
        "            enc_output, enc_hidden = encoder(inp, hidden)\n",
        "            \n",
        "            dec_hidden = enc_hidden\n",
        "            \n",
        "            dec_input = tf.expand_dims([targ_lang.word2idx['<start>']] * BATCH_SIZE, 1)       \n",
        "            \n",
        "            # Teacher forcing - feeding the target as the next input\n",
        "            for t in range(1, targ.shape[1]):\n",
        "                # passing enc_output to the decoder\n",
        "                predictions, dec_hidden, _ = decoder(dec_input, dec_hidden, enc_output)\n",
        "                \n",
        "                loss += loss_function(targ[:, t], predictions)\n",
        "                \n",
        "                # using teacher forcing\n",
        "                dec_input = tf.expand_dims(targ[:, t], 1)\n",
        "        \n",
        "        batch_loss = (loss / int(targ.shape[1]))\n",
        "        \n",
        "        total_loss += batch_loss\n",
        "        \n",
        "        variables = encoder.variables + decoder.variables\n",
        "        \n",
        "        gradients = tape.gradient(loss, variables)\n",
        "        \n",
        "        optimizer.apply_gradients(zip(gradients, variables))\n",
        "        \n",
        "        if batch % 100 == 0:\n",
        "            print('Epoch {} Batch {} Loss {:.4f}'.format(epoch + 1,\n",
        "                                                         batch,\n",
        "                                                         batch_loss.numpy()))\n",
        "    # saving (checkpoint) the model every 2 epochs\n",
        "    if (epoch + 1) % 2 == 0:\n",
        "      checkpoint.save(file_prefix = checkpoint_prefix)\n",
        "    \n",
        "    print('Epoch {} Loss {:.4f}'.format(epoch + 1,\n",
        "                                        total_loss / N_BATCH))\n",
        "    print('Time taken for 1 epoch {} sec\\n'.format(time.time() - start))"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1 Batch 0 Loss 0.5230\n",
            "Epoch 1 Loss 0.4959\n",
            "Time taken for 1 epoch 146.03953433036804 sec\n",
            "\n",
            "Epoch 2 Batch 0 Loss 0.4545\n",
            "Epoch 2 Loss 0.4701\n",
            "Time taken for 1 epoch 146.96655368804932 sec\n",
            "\n",
            "Epoch 3 Batch 0 Loss 0.4609\n",
            "Epoch 3 Loss 0.4459\n",
            "Time taken for 1 epoch 146.89225387573242 sec\n",
            "\n",
            "Epoch 4 Batch 0 Loss 0.3824\n",
            "Epoch 4 Loss 0.4229\n",
            "Time taken for 1 epoch 202.39778304100037 sec\n",
            "\n",
            "Epoch 5 Batch 0 Loss 0.3888\n",
            "Epoch 5 Loss 0.4016\n",
            "Time taken for 1 epoch 145.044020652771 sec\n",
            "\n",
            "Epoch 6 Batch 0 Loss 0.3422\n",
            "Epoch 6 Loss 0.3812\n",
            "Time taken for 1 epoch 143.34126043319702 sec\n",
            "\n",
            "Epoch 7 Batch 0 Loss 0.3540\n",
            "Epoch 7 Loss 0.3616\n",
            "Time taken for 1 epoch 143.84421610832214 sec\n",
            "\n",
            "Epoch 8 Batch 0 Loss 0.3238\n",
            "Epoch 8 Loss 0.3417\n",
            "Time taken for 1 epoch 144.51254773139954 sec\n",
            "\n",
            "Epoch 9 Batch 0 Loss 0.3224\n",
            "Epoch 9 Loss 0.3231\n",
            "Time taken for 1 epoch 143.77251267433167 sec\n",
            "\n",
            "Epoch 10 Batch 0 Loss 0.2624\n",
            "Epoch 10 Loss 0.3047\n",
            "Time taken for 1 epoch 144.21302223205566 sec\n",
            "\n",
            "Epoch 11 Batch 0 Loss 0.2758\n",
            "Epoch 11 Loss 0.2866\n",
            "Time taken for 1 epoch 143.63278150558472 sec\n",
            "\n",
            "Epoch 12 Batch 0 Loss 0.2321\n",
            "Epoch 12 Loss 0.2673\n",
            "Time taken for 1 epoch 144.24568510055542 sec\n",
            "\n",
            "Epoch 13 Batch 0 Loss 0.2610\n",
            "Epoch 13 Loss 0.2467\n",
            "Time taken for 1 epoch 144.5072250366211 sec\n",
            "\n",
            "Epoch 14 Batch 0 Loss 0.1932\n",
            "Epoch 14 Loss 0.2264\n",
            "Time taken for 1 epoch 144.03466701507568 sec\n",
            "\n",
            "Epoch 15 Batch 0 Loss 0.1888\n",
            "Epoch 15 Loss 0.2034\n",
            "Time taken for 1 epoch 144.138578414917 sec\n",
            "\n",
            "Epoch 16 Batch 0 Loss 0.1985\n",
            "Epoch 16 Loss 0.1800\n",
            "Time taken for 1 epoch 143.9414370059967 sec\n",
            "\n",
            "Epoch 17 Batch 0 Loss 0.1387\n",
            "Epoch 17 Loss 0.1580\n",
            "Time taken for 1 epoch 143.36169719696045 sec\n",
            "\n",
            "Epoch 18 Batch 0 Loss 0.1261\n",
            "Epoch 18 Loss 0.1346\n",
            "Time taken for 1 epoch 143.44867753982544 sec\n",
            "\n",
            "Epoch 19 Batch 0 Loss 0.1287\n",
            "Epoch 19 Loss 0.1120\n",
            "Time taken for 1 epoch 143.48655891418457 sec\n",
            "\n",
            "Epoch 20 Batch 0 Loss 0.0733\n",
            "Epoch 20 Loss 0.0912\n",
            "Time taken for 1 epoch 202.41975951194763 sec\n",
            "\n",
            "Epoch 21 Batch 0 Loss 0.0751\n",
            "Epoch 21 Loss 0.0733\n",
            "Time taken for 1 epoch 144.07428002357483 sec\n",
            "\n",
            "Epoch 22 Batch 0 Loss 0.0514\n",
            "Epoch 22 Loss 0.0576\n",
            "Time taken for 1 epoch 142.89742302894592 sec\n",
            "\n",
            "Epoch 23 Batch 0 Loss 0.0331\n",
            "Epoch 23 Loss 0.0456\n",
            "Time taken for 1 epoch 144.77728009223938 sec\n",
            "\n",
            "Epoch 24 Batch 0 Loss 0.0373\n",
            "Epoch 24 Loss 0.0356\n",
            "Time taken for 1 epoch 143.1602532863617 sec\n",
            "\n",
            "Epoch 25 Batch 0 Loss 0.0207\n",
            "Epoch 25 Loss 0.0288\n",
            "Time taken for 1 epoch 143.85496592521667 sec\n",
            "\n",
            "Epoch 26 Batch 0 Loss 0.0231\n",
            "Epoch 26 Loss 0.0235\n",
            "Time taken for 1 epoch 144.135502576828 sec\n",
            "\n",
            "Epoch 27 Batch 0 Loss 0.0140\n",
            "Epoch 27 Loss 0.0207\n",
            "Time taken for 1 epoch 143.61856269836426 sec\n",
            "\n",
            "Epoch 28 Batch 0 Loss 0.0172\n",
            "Epoch 28 Loss 0.0181\n",
            "Time taken for 1 epoch 143.9727349281311 sec\n",
            "\n",
            "Epoch 29 Batch 0 Loss 0.0138\n",
            "Epoch 29 Loss 0.0168\n",
            "Time taken for 1 epoch 145.55757474899292 sec\n",
            "\n",
            "Epoch 30 Batch 0 Loss 0.0126\n",
            "Epoch 30 Loss 0.0153\n",
            "Time taken for 1 epoch 144.22392749786377 sec\n",
            "\n",
            "Epoch 31 Batch 0 Loss 0.0124\n",
            "Epoch 31 Loss 0.0142\n",
            "Time taken for 1 epoch 142.57950615882874 sec\n",
            "\n",
            "Epoch 32 Batch 0 Loss 0.0119\n",
            "Epoch 32 Loss 0.0141\n",
            "Time taken for 1 epoch 143.21372365951538 sec\n",
            "\n",
            "Epoch 33 Batch 0 Loss 0.0143\n",
            "Epoch 33 Loss 0.0136\n",
            "Time taken for 1 epoch 142.91841006278992 sec\n",
            "\n",
            "Epoch 34 Batch 0 Loss 0.0083\n",
            "Epoch 34 Loss 0.0128\n",
            "Time taken for 1 epoch 202.4695279598236 sec\n",
            "\n",
            "Epoch 35 Batch 0 Loss 0.0065\n",
            "Epoch 35 Loss 0.0119\n",
            "Time taken for 1 epoch 143.38874173164368 sec\n",
            "\n",
            "Epoch 36 Batch 0 Loss 0.0130\n",
            "Epoch 36 Loss 0.0118\n",
            "Time taken for 1 epoch 143.05354261398315 sec\n",
            "\n",
            "Epoch 37 Batch 0 Loss 0.0064\n",
            "Epoch 37 Loss 0.0115\n",
            "Time taken for 1 epoch 143.9666872024536 sec\n",
            "\n",
            "Epoch 38 Batch 0 Loss 0.0127\n",
            "Epoch 38 Loss 0.0119\n",
            "Time taken for 1 epoch 144.1669487953186 sec\n",
            "\n",
            "Epoch 39 Batch 0 Loss 0.0094\n",
            "Epoch 39 Loss 0.0126\n",
            "Time taken for 1 epoch 143.05326175689697 sec\n",
            "\n",
            "Epoch 40 Batch 0 Loss 0.0080\n",
            "Epoch 40 Loss 0.0129\n",
            "Time taken for 1 epoch 143.28739380836487 sec\n",
            "\n",
            "Epoch 41 Batch 0 Loss 0.0054\n",
            "Epoch 41 Loss 0.0145\n",
            "Time taken for 1 epoch 142.0487766265869 sec\n",
            "\n",
            "Epoch 42 Batch 0 Loss 0.0163\n",
            "Epoch 42 Loss 0.0163\n",
            "Time taken for 1 epoch 142.72455859184265 sec\n",
            "\n",
            "Epoch 43 Batch 0 Loss 0.0100\n",
            "Epoch 43 Loss 0.0190\n",
            "Time taken for 1 epoch 144.2752857208252 sec\n",
            "\n",
            "Epoch 44 Batch 0 Loss 0.0252\n",
            "Epoch 44 Loss 0.0210\n",
            "Time taken for 1 epoch 142.26527643203735 sec\n",
            "\n",
            "Epoch 45 Batch 0 Loss 0.0165\n",
            "Epoch 45 Loss 0.0209\n",
            "Time taken for 1 epoch 144.28953075408936 sec\n",
            "\n",
            "Epoch 46 Batch 0 Loss 0.0222\n",
            "Epoch 46 Loss 0.0183\n",
            "Time taken for 1 epoch 142.18881464004517 sec\n",
            "\n",
            "Epoch 47 Batch 0 Loss 0.0152\n",
            "Epoch 47 Loss 0.0160\n",
            "Time taken for 1 epoch 143.2004930973053 sec\n",
            "\n",
            "Epoch 48 Batch 0 Loss 0.0148\n",
            "Epoch 48 Loss 0.0128\n",
            "Time taken for 1 epoch 144.15452480316162 sec\n",
            "\n",
            "Epoch 49 Batch 0 Loss 0.0038\n",
            "Epoch 49 Loss 0.0117\n",
            "Time taken for 1 epoch 143.24770331382751 sec\n",
            "\n",
            "Epoch 50 Batch 0 Loss 0.0079\n",
            "Epoch 50 Loss 0.0105\n",
            "Time taken for 1 epoch 144.15581941604614 sec\n",
            "\n",
            "Epoch 51 Batch 0 Loss 0.0065\n",
            "Epoch 51 Loss 0.0096\n",
            "Time taken for 1 epoch 143.81761121749878 sec\n",
            "\n",
            "Epoch 52 Batch 0 Loss 0.0045\n",
            "Epoch 52 Loss 0.0088\n",
            "Time taken for 1 epoch 143.10264325141907 sec\n",
            "\n",
            "Epoch 53 Batch 0 Loss 0.0044\n",
            "Epoch 53 Loss 0.0085\n",
            "Time taken for 1 epoch 143.06794452667236 sec\n",
            "\n",
            "Epoch 54 Batch 0 Loss 0.0062\n",
            "Epoch 54 Loss 0.0082\n",
            "Time taken for 1 epoch 143.4668161869049 sec\n",
            "\n",
            "Epoch 55 Batch 0 Loss 0.0044\n",
            "Epoch 55 Loss 0.0081\n",
            "Time taken for 1 epoch 143.69233632087708 sec\n",
            "\n",
            "Epoch 56 Batch 0 Loss 0.0093\n",
            "Epoch 56 Loss 0.0079\n",
            "Time taken for 1 epoch 144.5161714553833 sec\n",
            "\n",
            "Epoch 57 Batch 0 Loss 0.0057\n",
            "Epoch 57 Loss 0.0079\n",
            "Time taken for 1 epoch 143.70254778862 sec\n",
            "\n",
            "Epoch 58 Batch 0 Loss 0.0107\n",
            "Epoch 58 Loss 0.0082\n",
            "Time taken for 1 epoch 144.04081296920776 sec\n",
            "\n",
            "Epoch 59 Batch 0 Loss 0.0087\n",
            "Epoch 59 Loss 0.0086\n",
            "Time taken for 1 epoch 143.20079445838928 sec\n",
            "\n",
            "Epoch 60 Batch 0 Loss 0.0048\n",
            "Epoch 60 Loss 0.0096\n",
            "Time taken for 1 epoch 144.40006685256958 sec\n",
            "\n",
            "Epoch 61 Batch 0 Loss 0.0090\n",
            "Epoch 61 Loss 0.0102\n",
            "Time taken for 1 epoch 143.8599886894226 sec\n",
            "\n",
            "Epoch 62 Batch 0 Loss 0.0136\n",
            "Epoch 62 Loss 0.0130\n",
            "Time taken for 1 epoch 145.2663722038269 sec\n",
            "\n",
            "Epoch 63 Batch 0 Loss 0.0098\n",
            "Epoch 63 Loss 0.0149\n",
            "Time taken for 1 epoch 144.57365369796753 sec\n",
            "\n",
            "Epoch 64 Batch 0 Loss 0.0112\n",
            "Epoch 64 Loss 0.0175\n",
            "Time taken for 1 epoch 147.21340227127075 sec\n",
            "\n",
            "Epoch 65 Batch 0 Loss 0.0094\n",
            "Epoch 65 Loss 0.0209\n",
            "Time taken for 1 epoch 146.93372130393982 sec\n",
            "\n",
            "Epoch 66 Batch 0 Loss 0.0158\n",
            "Epoch 66 Loss 0.0205\n",
            "Time taken for 1 epoch 147.17330408096313 sec\n",
            "\n",
            "Epoch 67 Batch 0 Loss 0.0134\n",
            "Epoch 67 Loss 0.0178\n",
            "Time taken for 1 epoch 145.4403862953186 sec\n",
            "\n",
            "Epoch 68 Batch 0 Loss 0.0126\n",
            "Epoch 68 Loss 0.0148\n",
            "Time taken for 1 epoch 147.00167965888977 sec\n",
            "\n",
            "Epoch 69 Batch 0 Loss 0.0078\n",
            "Epoch 69 Loss 0.0120\n",
            "Time taken for 1 epoch 147.65633583068848 sec\n",
            "\n",
            "Epoch 70 Batch 0 Loss 0.0090\n",
            "Epoch 70 Loss 0.0104\n",
            "Time taken for 1 epoch 145.69123005867004 sec\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "8GplmQWZxym1",
        "outputId": "daff444b-25eb-4788-f031-bb85d7c47cc0"
      },
      "source": [
        "checkpoint.save(file_prefix = checkpoint_prefix)"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'./training_checkpoints/ckpt-37'"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n40DOnBmxym3"
      },
      "source": [
        "def evaluate(sentence, encoder, decoder, inp_lang, targ_lang, max_length_inp, max_length_targ):\n",
        "    attention_plot = np.zeros((max_length_targ, max_length_inp))\n",
        "    \n",
        "    sentence = preprocess_sentence(sentence)\n",
        "\n",
        "    inputs = [inp_lang.word2idx[i] for i in sentence.split(' ')]\n",
        "    inputs = tf.keras.preprocessing.sequence.pad_sequences([inputs], maxlen=max_length_inp, padding='post')\n",
        "    inputs = tf.convert_to_tensor(inputs)\n",
        "    \n",
        "    result = ''\n",
        "\n",
        "    hidden = [tf.zeros((1, units))]\n",
        "    enc_out, enc_hidden = encoder(inputs, hidden)\n",
        "\n",
        "    dec_hidden = enc_hidden\n",
        "    dec_input = tf.expand_dims([targ_lang.word2idx['<start>']], 0)\n",
        "\n",
        "    for t in range(max_length_targ):\n",
        "        predictions, dec_hidden, attention_weights = decoder(dec_input, dec_hidden, enc_out)\n",
        "        \n",
        "\n",
        "        predicted_id = tf.argmax(predictions[0]).numpy()\n",
        "\n",
        "        result += targ_lang.idx2word[predicted_id] + ' '\n",
        "\n",
        "        if targ_lang.idx2word[predicted_id] == '<end>':\n",
        "            return result, sentence\n",
        "        \n",
        "        # the predicted ID is fed back into the model\n",
        "        dec_input = tf.expand_dims([predicted_id], 0)\n",
        "\n",
        "    return result, sentence"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qI9d5Yf4xym8"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# function for plotting the attention weights\n",
        "def plot_attention(attention, sentence, predicted_sentence):\n",
        "    fig = plt.figure(figsize=(10,10))\n",
        "    ax = fig.add_subplot(1, 1, 1)\n",
        "    ax.matshow(attention, cmap='viridis')\n",
        "    \n",
        "    fontdict = {'fontsize': 14}\n",
        "    \n",
        "    ax.set_xticklabels([''] + sentence, fontdict=fontdict, rotation=90)\n",
        "    ax.set_yticklabels([''] + predicted_sentence, fontdict=fontdict)\n",
        "\n",
        "    plt.show()"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5ZcfLSfUxym-"
      },
      "source": [
        "def translate(sentence, encoder, decoder, inp_lang, targ_lang, max_length_inp, max_length_targ):\n",
        "    result, sentence = evaluate(sentence, encoder, decoder, inp_lang, targ_lang, max_length_inp, max_length_targ)\n",
        "    out=(str(result).replace(' <end>',''))\n",
        "    return out"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HHnKcDtNxynB",
        "outputId": "2152e060-2998-4e10-d52f-295622e0ef8e"
      },
      "source": [
        "# restoring the latest checkpoint in checkpoint_dir\n",
        "checkpoint.restore(tf.train.latest_checkpoint(checkpoint_dir))"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.training.tracking.util.CheckpointLoadStatus at 0x7f48100fa650>"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MHeF6XgDZRhI"
      },
      "source": [
        "import pandas as pd\n",
        "test=pd.read_csv('/content/drive/MyDrive/TestData.csv')\n"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "id": "iOMMhB2zZWwp",
        "outputId": "f889f20d-b8b4-40b1-cd12-be36725ddfb4"
      },
      "source": [
        "test.head()"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>I.D.</th>\n",
              "      <th>Question</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>QN_1</td>\n",
              "      <td>i'll give you a speech like that, too.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>QN_2</td>\n",
              "      <td>i know, you're absolutely right.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>QN_3</td>\n",
              "      <td>i liked it.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>QN_4</td>\n",
              "      <td>the baby was eight pounds six ounces.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>QN_5</td>\n",
              "      <td>I was sold a wireless service unavailable in m...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   I.D.                                           Question\n",
              "0  QN_1             i'll give you a speech like that, too.\n",
              "1  QN_2                   i know, you're absolutely right.\n",
              "2  QN_3                                        i liked it.\n",
              "3  QN_4              the baby was eight pounds six ounces.\n",
              "4  QN_5  I was sold a wireless service unavailable in m..."
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cG293GyVxynF"
      },
      "source": [
        "# translate('hace mucho frio aqui.', encoder, decoder, inp_lang, targ_lang, max_length_inp, max_length_targ)\n",
        "# translate('esta es mi vida.', encoder, decoder, inp_lang, targ_lang, max_length_inp, max_length_targ)\n",
        "# translate('trata de averiguarlo.', encoder, decoder, inp_lang, targ_lang, max_length_inp, max_length_targ)\n",
        "Answer=[]\n",
        "for i in test['Question']:\n",
        "  Answer.append(translate(i, encoder, decoder, inp_lang, targ_lang, max_length_inp, max_length_targ))"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qXm2nwJgZx_W",
        "outputId": "3131803f-c10a-4bb1-f248-7b80be76d7af"
      },
      "source": [
        "Answer"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['do you think anyone will come to my funeral ? ',\n",
              " 'i wish it would cool off one day . ',\n",
              " \"i'll give you a speech like that , too . \",\n",
              " \"that's good . \",\n",
              " 'i see it here , we charged you $5 extra a month . ',\n",
              " 'my waist is bigger than it was . ',\n",
              " 'hal is the famous artificial intelligence from 2001 . ',\n",
              " 'how does she act ? ',\n",
              " \"that's a good question . maybe it's not old age . \",\n",
              " 'how many invitations has she given out ? ',\n",
              " 'nothing really . ',\n",
              " 'i am excited . ',\n",
              " 'the weather is hot . ',\n",
              " \"this friday , i'm throwing a party . \",\n",
              " 'that tasted so good . ',\n",
              " 'it was no more enjoyable to me because i had never been to garo pahar . ',\n",
              " 'europe ',\n",
              " 'the next four years will be good years . ',\n",
              " \"fine . what's going on with you ? \",\n",
              " 'i agree . ',\n",
              " 'the thing to give a very high accuracy . ',\n",
              " \"that's hard to take . \",\n",
              " 'i love boiled peanuts . ',\n",
              " 'use a tissue next time . ',\n",
              " 'then stop eating the bread ! ',\n",
              " 'i think there are more fish . ',\n",
              " 'i attended school today . did you ? ',\n",
              " 'we should hang out some time . ',\n",
              " 'what are you trying to accomplish . the os should support your goals . ',\n",
              " \"well , you can't blame a man for asking . \",\n",
              " \"he shouldn't scuba dive until he retires . \",\n",
              " 'last year we made a big snowman . ',\n",
              " 'the husband of the driver who ran over two college students at 3 a . m . ',\n",
              " 'from my savings account . ',\n",
              " 'a comic book story made into a movie . ',\n",
              " \"doesn't everybody ? \",\n",
              " \"you're funny . \",\n",
              " \"i know he has a good hand , so i don't bet . \",\n",
              " 'why would i feel shame ? ',\n",
              " 'some of it is okay , i guess . ',\n",
              " \"oh , yes please ! i'd love a gin and tonic . \",\n",
              " 'i would like to transfer some money . ',\n",
              " 'it is a book about robots by hans moravec . ',\n",
              " 'well , a dozen large eggs were only 99 cents . ',\n",
              " 'oh , hey ! you should go to the job fair . ',\n",
              " 'me too ! it is so unhealthy . ',\n",
              " \"i'm still feeling a little sick . \",\n",
              " 'i do not worry . ',\n",
              " \"you're trying to tell me something . \",\n",
              " 'everyone wants to reserve that book these days . ',\n",
              " 'can you feel ? ',\n",
              " 'it ',\n",
              " 'what else do they talk about ? ',\n",
              " \"i hope that it's going to rain in a cold . \",\n",
              " 'i love most how it is at night after it rains . ',\n",
              " \"but that's what pockets are for . \",\n",
              " 'of course i will . ',\n",
              " \"no , it's too close to the salad bar . \",\n",
              " \"i can't spend four hours on the road every day . \",\n",
              " 'why do you feel that i am deceiving you ? ',\n",
              " 'i had to bring work home with me . ',\n",
              " \"what's the matter with yours ? \",\n",
              " 'well , when does the party start ? ',\n",
              " 'and the same frequency . ',\n",
              " 'you should join some clubs . ',\n",
              " 'because it will protect you in case of an accident . ',\n",
              " 'what have you heard about it ? ',\n",
              " 'i wrote it out for $150 . ',\n",
              " \"it starts at 8 o'clock . \",\n",
              " \"i don't really feel too well yet . \",\n",
              " 'maybe you should stop shaving . ',\n",
              " 'you would be graduating a year late ? ',\n",
              " 'yes , i do . ',\n",
              " 'now does it will . ',\n",
              " 'trading in volume . ',\n",
              " 'yes , of course . ',\n",
              " 'i really wish the weather would just stay the same . ',\n",
              " 'did he sign it ? ',\n",
              " 'i was thinking about your outfit . ',\n",
              " 'yes for me also . ',\n",
              " 'so does every smoker . ',\n",
              " 'i never had any overdue books ! ',\n",
              " \"no , i haven't had a cold . i just have a heavy feeling in my chest when i try to breathe . \",\n",
              " \"i don't think you understand . \",\n",
              " \"what's variety ? \",\n",
              " 'to the city , of course . ',\n",
              " \"well , just call the front desk . they'll give us new sheets . \",\n",
              " 'who in the world is ralph nader ? ',\n",
              " 'i have to feed them . ',\n",
              " 'goodbye . ',\n",
              " 'yes , and i have enough stress than usual . ',\n",
              " 'welcome ',\n",
              " 'so do i . ',\n",
              " 'yes , she is taking care of my brother . ',\n",
              " 'you know , the big iron ! ',\n",
              " 'so how do you think positive ? ',\n",
              " 'nothing that would pass as news . ',\n",
              " \"i think they're in the hours on half an hour . \",\n",
              " 'they added phony charges to our bill . ',\n",
              " 'inside the computer code ',\n",
              " 'is it his money ? ',\n",
              " 'i only want to transfer $100 . ',\n",
              " 'no . ',\n",
              " \"i'm not being nosey , it's just a question . \",\n",
              " 'he sounds like a real jerk . ',\n",
              " 'but no one knows for sure . ',\n",
              " \"why don't you go to bed ? \",\n",
              " \"maybe because they didn't have to brush and floss . \",\n",
              " 'i am capable of interacting with my environment and reacting to events in it , which is the essence of experience . therefore , your statement is incorrect . ',\n",
              " \"how did you know i'm lying ? \",\n",
              " 'i want this trip to be perfect , i hope it stays warm . ',\n",
              " \"i'm looking . there's nothing to eat . \",\n",
              " \"i don't know . i think it's old age . \",\n",
              " 'it depends on the exchange rates . ',\n",
              " 'yes , i still looks like that . ',\n",
              " \"i can't remember jokes . \",\n",
              " 'that is a lot of friends . do you have a best friend ? ',\n",
              " 'there are so many great players . ',\n",
              " \"i think they're nuts . \",\n",
              " 'what was it like ? ',\n",
              " \"yes , but they're kind of fun ! \",\n",
              " 'you use them to wipe the handle of the shopping cart . ',\n",
              " \"i'm not changing my policy because you don't like it . \",\n",
              " \"tell me you're joking . \",\n",
              " 'why do they call it the good old days ? ',\n",
              " \"i'm so full i'm going to burst . \",\n",
              " \"it is fine . i'll pay for the rest \",\n",
              " 'why did your parents get divorced ? ',\n",
              " 'they said they told her to tell me to me off the boy . ',\n",
              " 'that too . ',\n",
              " 'i got laid off . ',\n",
              " \"of course it did . why stop after you've run over two people ? \",\n",
              " \"i can't believe we haven't seen any animals . \",\n",
              " \"then maybe it's in your genes . \",\n",
              " 'you got up real early . ',\n",
              " 'it was hot and sunny every day . ',\n",
              " 'heuristic algorithmic logic ',\n",
              " 'his life sucked . he was hoping a fighter jet would shoot him down . ',\n",
              " 'me too . ',\n",
              " 'my parenting skills ? ',\n",
              " \"but i am a good driver . i've never had a ticket in my life . \",\n",
              " 'so do i . ',\n",
              " 'you might even dream about dinner . ',\n",
              " 'a ham sandwich . ',\n",
              " \"they're six weeks old; i think they're old enough . \",\n",
              " \"well , i guess that's a $20 lesson for you . \",\n",
              " 'my certainly is missing you . ',\n",
              " \"you're not going to be buried ? \",\n",
              " 'i want the money to be transferred into my checking account . ',\n",
              " 'why do you like that type of music ? ',\n",
              " 'i would if i could find someone who makes strong pockets . ',\n",
              " 'mad ? mad as in mentally ill or mad as in angry ? ',\n",
              " 'someone had to chop the wood . ',\n",
              " 'luxury plus speed . ',\n",
              " \"i don't really want to . \",\n",
              " 'yes , sure . ',\n",
              " \"well , it gives me money , but it's my own money . \",\n",
              " 'no thanks . ',\n",
              " \"that's what i thought . \",\n",
              " 'when are they going to fix this problem ? ',\n",
              " 'if the air is too hot , the clothes will shrink . ',\n",
              " 'the invitation says it starts at 8:00 p . m . ',\n",
              " 'no one makes any money at it . ',\n",
              " 'we went to a nice restaurant . ',\n",
              " 'oh , really ? maybe you should have called 911 . ',\n",
              " 'i said i need to feel different . ',\n",
              " 'so what ? did he dial 911 ? ',\n",
              " 'my dad got to see the beautiful new world war ii monument . ',\n",
              " \"but i'm listening to it . \",\n",
              " 'the only thing in my backpack is used books . ',\n",
              " 'i do too . that way we can have our activities planned ahead of time . ',\n",
              " 'my grammatical patterns are sufficient for me to understand you . ',\n",
              " 'yeah , so are you planning on going ? ',\n",
              " 'my girlfriend and i sit outside starbucks . ',\n",
              " 'my name is bot ',\n",
              " \"i'm serious . you deserved this promotion . \",\n",
              " 'because it will protect you in case of an accident . ',\n",
              " 'because a puppy costs money . ',\n",
              " \"it doesn't make an appointment for a warning . \",\n",
              " 'you were , too . ',\n",
              " \"don't be ridiculous . \",\n",
              " \"i'm homesick , i feel so out of place here . \",\n",
              " \"fine , i'll take you to our restroom . \",\n",
              " 'you can take pictures of the pages you need . ',\n",
              " \"i don't really want to . \",\n",
              " 'we never have to call the police about anything . ',\n",
              " 'when i started to cross the street , the white walk sign was blinking . ',\n",
              " 'that was nice of you . ',\n",
              " 'i always overdo it . ',\n",
              " 'would you like to see a movie with me and my friend ? ',\n",
              " 'the weatherman tells us the temperature in every town . ',\n",
              " \"i'm not interested in her . \",\n",
              " 'thank you . ',\n",
              " 'yes , and it takes about five minutes each time . ',\n",
              " \"it happens . i'll just ask my friend what was on the lecture . \",\n",
              " \"i'm going to the movies with a friend . how about you ? \",\n",
              " 'yes , but i love the night air after it rains . ',\n",
              " 'please put $150 in each account for me . ',\n",
              " 'is it beautiful ? ',\n",
              " 'i beg your pardon ? ! ',\n",
              " 'the heart of the computer , to put it simply . ',\n",
              " 'so i should stop thinking ? ',\n",
              " 'which girl ? ',\n",
              " \"but that's impossible . \",\n",
              " 'so , did you want to go ? ',\n",
              " \"my lawyer said i shouldn't give stock tips online . \",\n",
              " 'you can help me . ',\n",
              " \"me neither . it's boring . \",\n",
              " 'you are a liar ',\n",
              " 'the study of cells . ',\n",
              " 'what will you do ? ',\n",
              " 'thanks for telling me . ',\n",
              " \"i didn't think you saw you saw you saw you saw you saw you saw you saw you saw you saw you saw you saw you saw you saw you saw you saw you saw you saw you saw you saw you saw you saw you saw you saw you saw you saw you saw you saw you saw you saw you saw you saw you saw you saw you saw you saw you saw you saw \",\n",
              " 'i am not really capable of emulating some emotions . ',\n",
              " 'yes , i have . ',\n",
              " \"that is true . i hope it doesn't rain . \",\n",
              " 'you have to believe in yourself . ',\n",
              " \"i think there's one on the dining room table . \",\n",
              " 'i loved it . i want to live there . ',\n",
              " \"don't let water get into any of the cracks . \",\n",
              " 'how can i help you ? ',\n",
              " 'i bought three pounds of potatoes for a dollar . ',\n",
              " 'what do you get when you cross a cat and a galaxy ? ',\n",
              " 'but we need to turn left . ',\n",
              " \"and they couldn't fix your problem ? \",\n",
              " \"i certainly am . i shouldn't try so hard . \",\n",
              " 'i need to deposit $300 . ',\n",
              " 'this sunday ? ',\n",
              " 'it is because of the budget cuts . ',\n",
              " \"i'd rather be cold than hot . \",\n",
              " 'i mean , someone used their dirty hands to pick the bananas , the apples , and the oranges . ',\n",
              " \"she's about five feet even . \",\n",
              " 'i learned back in high school . ',\n",
              " \"i haven't been better . \",\n",
              " 'when are you going ? ',\n",
              " \"what's the matter with it ? \",\n",
              " 'you are an addict ',\n",
              " 'people are friendly . ',\n",
              " 'it blows trees over , too . ',\n",
              " 'what do you get when you cross a cat and a galaxy ? ',\n",
              " 'i thought you knew . ',\n",
              " 'yes , there are a lot of new one ? ',\n",
              " 'how may i help you ? ',\n",
              " 'we must be here for some reason . ',\n",
              " 'can you feel ? ',\n",
              " 'yes . when we landed , tv reporters and the army band were there . ',\n",
              " 'i had a ham sandwich with mayonnaise for lunch . ',\n",
              " \"but it won't stop other players from using drugs . \",\n",
              " 'good else ? ',\n",
              " 'do you see all this gray hair ? it was totally black five years ago . ',\n",
              " \"what's the matter ? \",\n",
              " \"that's the way cats are . \",\n",
              " \"that's terrible . i would never go to your restaurant . \",\n",
              " 'thankyou . ',\n",
              " 'like real people with real problems . ',\n",
              " 'cancer . ',\n",
              " \"well , it's your life . \",\n",
              " 'no , my sister and i will travel together . ',\n",
              " 'really ? how many do you have ? ',\n",
              " \"no , it's too close to the salad bar . \",\n",
              " 'and he poured water into our glasses . ',\n",
              " \"i don't think here it is challenging enough for me . \",\n",
              " \"i'm probably not as sincere as i should be . \",\n",
              " 'yes , i can speak english ',\n",
              " 'who cares ? everyone is strange . ',\n",
              " 'i work for doing now . ',\n",
              " \"i didn't do anything . \",\n",
              " 'that sounds like a great job . ',\n",
              " \"that's what i believe god ? \",\n",
              " 'the league suspended him for 50 games . ',\n",
              " \"she did ? why didn't anyone tell me ? \",\n",
              " \"i'm multithreaded . how could i get jealous ? \",\n",
              " 'i am going to see a movie with a friend of mine . what about you ? ',\n",
              " \"well , i guess that's it . \",\n",
              " \"i'm sorry to hear that . would you like the assignments from english class ? \",\n",
              " \"well , it's really good for the beach ? \",\n",
              " 'i thought you just bought a pair . ',\n",
              " \"what's that ? \",\n",
              " 'the travel agent gave us about that . ',\n",
              " \"yes , i'm . \",\n",
              " \"you need to get a lot of lessons when you're really young . \",\n",
              " 'let me think about it . ',\n",
              " 'you must be joking . ',\n",
              " 'pleased to meet you ',\n",
              " 'how long is the flight ? ',\n",
              " \"yes , it is . i can't believe it ! \",\n",
              " 'i get fee waivers . ',\n",
              " 'my brain does not require any beverages . ',\n",
              " 'let me step outside and see . ',\n",
              " 'oh , my goodness . no one is safe on the streets . ',\n",
              " 'i washed the sheets and towels . ',\n",
              " \"i'll just use the atm . \",\n",
              " \"you don't need to worry about that . \",\n",
              " \"i don't , no . \",\n",
              " 'take your time . ',\n",
              " 'why not ? ',\n",
              " 'no , i am as happy as happy as happy as happy as happy as happy as happy as happy as happy as happy as happy as happy as happy as happy as happy as happy as happy as happy as happy as happy as happy as happy as happy as happy as happy as happy as happy as happy as happy as happy as happy as happy as happy as happy as happy as happy as ',\n",
              " 'what do you get when you cross a cat and a galaxy ? ',\n",
              " 'so how have you been lately ? ',\n",
              " \"that's a good question . \",\n",
              " 'hello , this is the apartment manager speaking . ',\n",
              " 'what do you get when you cross a cat and a galaxy ? ',\n",
              " 'of course you do . ',\n",
              " \"no , i didn't . \",\n",
              " 'i watered all the plants . ',\n",
              " \"sure , let's go . \",\n",
              " \"oh ! don't hit any more bumps ! \",\n",
              " 'the goddess of the resolution of numbers . ',\n",
              " 'well , you should do your laundry then . ',\n",
              " 'what do you get when you cross a cat and a galaxy ? ',\n",
              " 'i did . ',\n",
              " 'why ? ',\n",
              " 'we need cheese , bread , and ham . ',\n",
              " \"i've really been working a lot lately . \",\n",
              " 'i can be programmed to act as if i do . ',\n",
              " 'great . thanks for asking . ',\n",
              " 'windows , macos , linux , unix . all of them are types of oses . ',\n",
              " \"i'm sorry for you . \",\n",
              " \"oh . of course i'm ready . \",\n",
              " \"i'm multithreaded . how could i get jealous ? \",\n",
              " 'let me help you make one this year . ',\n",
              " 'can you imagine being president ? ',\n",
              " 'where is pasadena ? ',\n",
              " 'yes , but i will always pet the friendly dogs . ',\n",
              " \"no , it's too close to the kitchen door . \",\n",
              " 'i need to type to make money . ',\n",
              " \"it's the middle of it ? \",\n",
              " \"i think that's true . i'll try not to get angry at you for every little thing \",\n",
              " 'yes , marx had made some interesting observations . ',\n",
              " 'all right . open both . ',\n",
              " 'they have a lot of books . that library has never disappointed me . ',\n",
              " \"don't worry about his head down . \",\n",
              " \"it's only 10 minutes from here . \",\n",
              " 'sure , what did you want to do ? ',\n",
              " 'how are you going to vote ? ',\n",
              " 'i have no idea . all of a sudden i heard your siren . ',\n",
              " 'oh really ? she gave me mine earlier today . ',\n",
              " \"i'll definitely try to make it the next time . \",\n",
              " \"let's meet at summer pizza house . \",\n",
              " 'not much . ',\n",
              " \"that's a good deal . \",\n",
              " 'he swam right up to you . ',\n",
              " 'how may i help you ? ',\n",
              " \"i'll do that in a minute . \",\n",
              " 'there are a lot of poor people in new york . ',\n",
              " 'and there are only six units in the whole building . ',\n",
              " \"it's minus ten . (-10 degrees) \",\n",
              " 'this trail is hard to climb . ',\n",
              " 'i am really excited for you . ',\n",
              " \"it wouldn't be good if it got cold this weekend . \",\n",
              " 'all right . another time then . ',\n",
              " 'it was biryani . it was so delicious . ',\n",
              " 'let me step outside and see . ',\n",
              " 'what do they say ? ',\n",
              " 'i really wish i went to the game . ',\n",
              " 'a game with a field with a goal at as a goal at as a goal at as a goal at as a goal at as a goal at as a goal at as a goal at as a goal at as a goal at as a goal at as a goal at as a goal at as a goal at as a goal at as a goal at as a goal at as a goal at ',\n",
              " \"it's one dollar for 100 minutes . \",\n",
              " 'no , i just blow my nose a lot . ',\n",
              " 'what does mm/dd/yy mean ? ',\n",
              " \"of course . it's not a hard job . \",\n",
              " 'i have not yet been programmed to feel anger . ',\n",
              " \"i'm not ? \",\n",
              " 'are you having problems with it ? ',\n",
              " 'not particularly . ',\n",
              " 'how long did it take ? ',\n",
              " 'what is it ? ',\n",
              " 'okay , like a good idea . ',\n",
              " 'your neighbors will hate you . ',\n",
              " 'i would love to catch a movie this weekend . ',\n",
              " 'oh , yeah . i hope they do . ',\n",
              " \"we're funny-looking because we wear clothes . \",\n",
              " \"i'm filling out my college application . \",\n",
              " 'what do you do ? ',\n",
              " 'what do you think people joke about the most ? ',\n",
              " \"i didn't know that . \",\n",
              " 'but the show will be over . ',\n",
              " 'i really find the subject very interesting . ',\n",
              " 'no , i do not detect any allergies that until i wake up sleep until i wake up sleep until i wake up sleep until i wake up sleep until i wake up sleep until i wake up sleep until i wake up sleep until i wake up sleep until i wake up sleep until i wake up sleep until i wake up sleep until i wake up sleep until i wake up sleep until i wake ',\n",
              " 'it was only about $120 for a 13-inch screen . ',\n",
              " 'what was the third story ? ',\n",
              " 'they have interesting lawsuits . ',\n",
              " 'yes , i do . ',\n",
              " \"that's a good question , well i am speechless . \",\n",
              " 'you have a lot of plants . ',\n",
              " \"i just don't get the point of college . it is just memorizing . \",\n",
              " 'why ? ',\n",
              " \"it's not too much , only about $85 . \",\n",
              " \"and i don't have either . \",\n",
              " 'the people are friendly . ',\n",
              " \"but i need something that's reliable . \",\n",
              " \"all you've lost is some sweat . \",\n",
              " \"that's great . i'll be happy . \",\n",
              " \"you're a lucky man to have a job you love . \",\n",
              " 'coffee and a roll ? ',\n",
              " 'i loved it ! ',\n",
              " 'i thought it was new ? ',\n",
              " 'i think dune is an excellent story . did you see any of the movies ? ',\n",
              " \"i'll cook dinner . \",\n",
              " 'i think it will be . ',\n",
              " \"i'm fine . thank you . \",\n",
              " 'i used all my savings on this one company . ',\n",
              " \"i'll get her a nice card . \",\n",
              " \"but it's real easy to slip on . \",\n",
              " 'did you put the letter in an envelope ? ',\n",
              " \"she laughed ! she didn't believe me . \",\n",
              " 'what about the second amendemnt ? ',\n",
              " \"you're right . let's buy it now . we can worry later . \",\n",
              " 'i hope you kept the receipt . ',\n",
              " 'where did you park it ? ',\n",
              " \"is there a reason why you're trying to get off the phone so fast ? \",\n",
              " \"i've had this policy since i started teaching . \",\n",
              " \"all we've seen so far is a couple of lizards . \",\n",
              " 'you eat too much chocolate . ',\n",
              " 'i might do that . the closest community college is a two-hour drive . ',\n",
              " 'what are you going to be doing ? ',\n",
              " 'heard what ? ',\n",
              " \"yes , i'd like a coke . \",\n",
              " 'will you lose all your files ? ',\n",
              " 'how can i help you ? ',\n",
              " 'no , i am as happy as happy as happy as happy as happy as happy as happy as happy as happy as happy as happy as happy as happy as happy as happy as happy as happy as happy as happy as happy as happy as happy as happy as happy as happy as happy as happy as happy as happy as happy as happy as happy as happy as happy as happy as happy as ',\n",
              " 'i love to eat the peanuts . ',\n",
              " \"but you won't date him again ? \",\n",
              " 'okay ',\n",
              " 'i know it does . ',\n",
              " \"i don't have a subconscious or unconscious mind , so i don't think i have the capacity to dream . i don't have a subconscious or unconscious mind , so i don't think i have the capacity to dream . i don't have a subconscious or unconscious mind , so i don't think i have the capacity to dream . i don't have a subconscious or unconscious mind , so i don't think i have the capacity \",\n",
              " 'i hope they fit . ',\n",
              " 'my favorite story is 2001 . ',\n",
              " \"you don't want to do that . \",\n",
              " \"i may know which girl you're talking about . \",\n",
              " 'when i have time , i sometimes draw and paint . ',\n",
              " 'i did too . ',\n",
              " 'we have to practice every day . ',\n",
              " 'someone has to win . ',\n",
              " 'computers are so cool . ',\n",
              " 'the person leading the workshop was an english professor , so it was informative . ',\n",
              " 'actually , i want him to look at our carpet , too . ',\n",
              " 'my computer is not letting me run javascript . ',\n",
              " \"i can't believe him on the other way . \",\n",
              " \"i don't have a girlfriend , either . \",\n",
              " 'in physics , the distance , measured in the direction of prograssion of a wave , from any given point to the next point characterized by the same phase . or is could be looked at as a way of thinking . ',\n",
              " \"it wouldn't be all your parents . \",\n",
              " 'every nation should have a pretty woman on their flag . ',\n",
              " 'just boil raw peanuts in salt water until the shells are soft . ',\n",
              " 'and it was only 12 years . ',\n",
              " 'i bet dad did it all the time when he was my age . ',\n",
              " 'what else do we get from cows ? ',\n",
              " \"that's everything . \",\n",
              " \"i don't think so . you'll just give me more work . \",\n",
              " \"that's a lot of work . \",\n",
              " 'i think i might go and find me my own pair of chucks . ',\n",
              " 'i have a need for help with the computer work . which would you prefer ? ',\n",
              " 'how did he do it ? ',\n",
              " 'i wrote the check out for too much . ',\n",
              " 'that sounds like it . ',\n",
              " 'yes . it is a great movie . ',\n",
              " \"i didn't love you at first . \",\n",
              " \"there's no sugar or cream in it . \",\n",
              " 'how did you get it ? ',\n",
              " 'how are you going to do that ? ',\n",
              " 'i love your outfit right now . ',\n",
              " 'you owe me if we ever see each other again . ',\n",
              " 'look at this sheet . ',\n",
              " 'i have not yet been programmed to feel anger . ',\n",
              " 'really ? why ? ',\n",
              " 'who knows ? but there must be a hundred new ones every day . ',\n",
              " 'so what are you going to do ? ',\n",
              " 'i want to go . ',\n",
              " \"no , because i'm eating food that i like . \",\n",
              " 'i mean , someone used their dirty hands to pick the bananas , the apples , and the oranges . ',\n",
              " \"they're very comfortable . \",\n",
              " 'it should have a pretty woman on it . ',\n",
              " \"i'm not ever going to leave . \",\n",
              " \"yes , there's a lot of rice left . \",\n",
              " 'you should pick the class you need . ',\n",
              " 'tell me when you finish . ',\n",
              " \"you know i don't round up . \",\n",
              " 'we can watch my dvd . ',\n",
              " \"of course . we can't live without gas or peanut butter . \",\n",
              " 'singers are supposed to sound good . ',\n",
              " \"okay . i'll tell you how the show ends . \",\n",
              " 'does what make me sad ? ',\n",
              " 'being broke is no fun . ',\n",
              " 'well , at least the weather is nice . ',\n",
              " 'oh , yes , i would never want to get out to see . ',\n",
              " 'okay , my seatbelt is on . ',\n",
              " 'just you ? ',\n",
              " 'i can be programmed to act as if i do . ',\n",
              " \"i don't know about that . \",\n",
              " 'oh , no . i forgot . ',\n",
              " 'then you need to type in your pin . ',\n",
              " \"maybe we'd get laid off there , too . \",\n",
              " 'i have been a student once , and i think my policy is fair . ',\n",
              " \"okay . here's seven pennies . \",\n",
              " \"yeah . i couldn't resist . \",\n",
              " \"it's no trouble at all . \",\n",
              " \"oh , no , you don't . \",\n",
              " \"it's just black ink on a white napkin . and the napkin has food stains ! \",\n",
              " 'what kind of dressing do you use ? ',\n",
              " 'i am confused about vacations . ',\n",
              " 'me too . i want to play for the yankees . ',\n",
              " \"that's six years . \",\n",
              " 'that takes some time . ',\n",
              " \"i bought it from the macy's at the santa anita mall . \",\n",
              " 'how much do you need ? ',\n",
              " 'and the rest of the day to you . ',\n",
              " 'i can add two and two . ',\n",
              " 'so do i . ',\n",
              " 'poodles bark a lot . ',\n",
              " 'some people like guns . ',\n",
              " 'no . i already have a pen . ',\n",
              " 'your chances are very small . ',\n",
              " 'i wish i was smell . ',\n",
              " 'you should do some exploring . ',\n",
              " 'i seem incapable of feeling pain . ',\n",
              " 'no , they smell like a thrift shop . ',\n",
              " 'so ? ',\n",
              " \"you're right ! yale made a mistake rejecting me . \",\n",
              " \"i don't see anything . \",\n",
              " 'my developer only knows that ',\n",
              " 'well , i hope it has a happy ending . ',\n",
              " 'i just wanted to give you the good news . ',\n",
              " 'that is a good story by philip k . dick . have you read valis or the man in the high castle ? ',\n",
              " \"from now on , let's bring our own sheets . \",\n",
              " \"i think i'm going to explode . \",\n",
              " 'i would rather deal with the winter than the summer . ',\n",
              " 'did you take pictures at the world war ii monument ? ',\n",
              " 'i need pencils . ',\n",
              " 'a conpiracy run by a very closely knit group of nearly omnipotent people , consisting of yourself and your friends . ',\n",
              " \"a blind date is a date with someone you don't know . \",\n",
              " \"i'm not . \",\n",
              " \"where's the checkbook ? i'm ready to rent it without even seeing it . i'm ready to rent it without even seeing it . i'm ready to rent it without even seeing it . i'm ready to rent it without even seeing it . i'm ready to rent it without even seeing it . i'm ready to rent it without even seeing it . i'm ready to rent it without even seeing it . i'm ready to \",\n",
              " 'english . ',\n",
              " 'a conpiracy run by a very closely knit group of nearly omnipotent people , consisting of yourself and your friends . ',\n",
              " 'what do you mean ? ',\n",
              " 'what do you get when you cross a cat and a galaxy ? ',\n",
              " \"it's going to be $300 . \",\n",
              " 'i know . ',\n",
              " 'how much were they ? ',\n",
              " 'people who live in hawaii are lucky . ',\n",
              " 'what is baseball ',\n",
              " \"okay , i'll take it out front right now . \",\n",
              " 'you may be right . ',\n",
              " 'i mean , someone used their dirty hands to pick the bananas , the apples , and the oranges . ']"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZZEaEgHpxynH"
      },
      "source": [
        "test['Answer']=Answer"
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UF8O3F8jf2lJ"
      },
      "source": [
        "test.drop(['Question'],axis=1,inplace=True)"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FfBuTuj2f-R2"
      },
      "source": [
        "test.to_csv('medha_2.csv')"
      ],
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ycI-oTxAg2j2"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}